# -*- coding: utf-8 -*-
"""Untitled89.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14OgQQRw2AOs6E0u_-Nuw0_AUL5_JT6Ji
"""

# 1.1.1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

df = pd.read_csv("/content/diabetes.csv")

#EDA
print(df.head())

print (df.describe())

#Finding missing value
print(df.isnull().sum())

# 1.2.2. Analyze Target Balance
print("\n--- Target Variable Analysis ('Outcome') ---")
outcome_counts = df['Outcome'].value_counts()
print("Outcome Value Counts (0=Not Diabetic, 1=Diabetic):\n", outcome_counts)

# Calculate percentage
total_patients = len(df)
percentage_diabetic = (outcome_counts[1] / total_patients) * 100
percentage_non_diabetic = (outcome_counts[0] / total_patients) * 100
print(f"Percentage of Diabetic Patients (1): {percentage_diabetic:.2f}%")
print(f"Percentage of Non-Diabetic Patients (0): {percentage_non_diabetic:.2f}%")
# Self-Check: The dataset is not perfectly balanced, as 0 and 1 counts are different. [cite: 25]

# 1.2.3. Visualize Distributions
plt.figure(figsize=(15, 5))

# Histogram for Glucose
plt.subplot(1, 3, 1)
sns.histplot(df['Glucose'], kde=True)
plt.title('Distribution of Glucose')

# Histogram for BMI
plt.subplot(1, 3, 2)
sns.histplot(df['BMI'], kde=True)
plt.title('Distribution of BMI')

# Count plot for Outcome
plt.subplot(1, 3, 3)
sns.countplot(x='Outcome', data=df)
plt.title('Count Plot of Outcome Variable')
plt.show()

# Step 2.1: Handling Missing Data (Zero Imputation)

# Identify columns with suspicious 0 values [cite: 31]
cols_to_impute = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']

# 2.1.1. Count Suspicious Zeros
print("\n--- Counting Suspicious Zero Values ---")
for col in cols_to_impute:
    zero_count = (df[col] == 0).sum()
    print(f"'{col}': {zero_count} zero values found.")

# 2.1.2. Imputation: Replace 0s with the median of the respective column [cite: 33]
for col in cols_to_impute:
    # Calculate the median, ignoring 0s (by replacing them with NaN temporarily,
    # but the simplest approach is to replace with the median of the entire column as per instructions)
    median_val = df[col].median()
    df[col] = df[col].replace(0, median_val)

print("\nZero values in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] replaced with the median.")

# 2.1.3. Verify Cleaning
print("\n--- Descriptive Statistics (After Cleaning) ---")
print(df.describe()) # Observe that the minimum values for the imputed columns are no longer 0 [cite: 38, 39]


# ---
# Step 2.2: Prepare Data for Modeling and Scaling

# 2.2.1. Define X and y: Separate features (X) from the target (y) [cite: 41]
X = df.drop('Outcome', axis=1) # Features [cite: 42]
y = df['Outcome']             # Target [cite: 43]
print("\nFeatures (X) and Target (y) defined.")

# 2.2.2. Train-Test Split: Split data into 70% training and 30% testing [cite: 44]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42 # Set random_state for reproducibility [cite: 45, 48]
)
print(f"Data Split: Training size: {len(X_train)}, Testing size: {len(X_test)}")

# 2.2.3. Feature Scaling (Standardization)
# Initialize the StandardScaler [cite: 50, 55]
scaler = StandardScaler()

# Fit the scaler ONLY on the training data (X_train) [cite: 51, 56]
X_train_scaled = scaler.fit_transform(X_train)

# Transform both the training and testing data using the fitted scaler [cite: 52, 57]
X_test_scaled = scaler.transform(X_test)
print("Features have been standardized.")

# Step 3.1: Train the Classification Model

# 3.1.1. Instantiate Model: Initialize the Logistic Regression classifier [cite: 60, 64]
model = LogisticRegression(random_state=42)

# 3.1.2. Train Model: Fit the model using scaled training data [cite: 61, 65]
model.fit(X_train_scaled, y_train)
print("\nLogistic Regression Model trained successfully.")

# ---
# Step 3.2: Make Predictions

# Prediction: Predict on the scaled test data [cite: 67, 68]
y_pred = model.predict(X_test_scaled)

# ---
# Step 3.3: Evaluate Performance

# 3.3.1. Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("\n--- Model Evaluation Metrics ---")
print(f"Accuracy Score: {accuracy:.4f}") # Calculate overall prediction accuracy [cite: 70, 74]

# 3.3.2. Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:\n", conf_matrix) # Generate the Confusion Matrix [cite: 71, 75]
# Interpretation of Confusion Matrix (Row: Actual, Column: Predicted):
# [[True Negative (TN) | False Positive (FP)]
#  [False Negative (FN) | True Positive (TP)]]

# 3.3.3. Classification Report
class_report = classification_report(y_test, y_pred)
print("\nClassification Report:\n", class_report) # Print Precision, Recall, F1-Score for each class [cite: 76, 78]

